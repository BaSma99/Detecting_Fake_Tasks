# -*- coding: utf-8 -*-
"""Smart_Citeis_Group_8_Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QQLXNP61xFrqyqAjio5tMKDmJDUJS05E

**Smart Citeis Group 8 Assignment 2**

## **Import important libraries and load the dataset**
"""

import pandas as pd #data analysis library
import numpy as np  #to deal with arrays

#read the dataset
df = pd.read_csv('/content/MCSDatasetNEXTCONLab.csv')
df

#get some information about the data
df.info()

#to show the dataset columns
df.columns

#counting values of Ligitimacy column
df['Ligitimacy'].value_counts()

# as we can see we have imbalance dataset
import seaborn as sns
sns.countplot(df['Ligitimacy'])

import plotly.express as px #to create entire figures at once
fig = px.scatter_geo(df,
                    lat=df['Latitude'],
                    lon=df['Longitude'],
                    )
fig.show()

# define input and output
x= df.drop('Ligitimacy',axis = 1)
y= df['Ligitimacy']

# x=df.iloc[:,:12]
# y=df.iloc[:,-1]
# x

"""## **Split the dataset into train(80%) and test(20%) split** """

from sklearn.model_selection import train_test_split
x_train,x_test , y_train , y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)

"""## **Random Forest classifier**

A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.
"""

#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier,VotingClassifier

#Create a Gaussian Classifier
rl=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
rl.fit(x_train,y_train)

y_pred_rl=rl.predict(x_test)

#print the classification report after applying random forest classifier
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix ,plot_confusion_matrix
from sklearn.metrics import classification_report 
print(classification_report(y_train,rl.predict(x_train)))
print(classification_report(y_test,y_pred_rl))
plt.show()

"""The accuracy of training is 100% and accuracy of testing is 99%"""

print(confusion_matrix(y_test, y_pred_rl))

from sklearn.metrics import accuracy_score
X=accuracy_score(y_train, rl.predict(x_train))
#acuuracy for test
acc_RL= accuracy_score(y_test, y_pred_rl)
print(acc_RL)

"""## **Apply Adaboost classification Model**

AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations.
"""

# Create adaboost classifer object
from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier(n_estimators=100,learning_rate=1)
#Train the model using the training sets y_pred=clf.predict(X_test)
ada.fit(x_train,y_train)

y_pred_ada=ada.predict(x_test)

print(classification_report(y_train,ada.predict(x_train)))
print(classification_report(y_test,y_pred_ada))

"""The training accuracy is 97% and testing accuracy is 97%"""

print(confusion_matrix(y_test, y_pred_ada))

Y=accuracy_score(y_train, ada.predict(x_train))
#acuuracy for test
acc_ADA= accuracy_score(y_test, y_pred_ada)
print(acc_ADA)

"""#**Apply GaussianNB classification Model**

A Gaussian Naive Bayes algorithm is a special type of NB algorithm. Itâ€™s specifically used when the features have continuous values.
"""

#to import GaussianNB classifier model
from sklearn.naive_bayes import GaussianNB 
nb = GaussianNB()
nb.fit(x_train,y_train)
#define y_pred to make the prediction
y_pred_nb = nb.predict(x_test)

print(classification_report(y_train,nb.predict(x_train)))
print(classification_report(y_test,y_pred_nb))

"""The training accuracy is 87% and testing accuracy is 78%"""

print(confusion_matrix(y_test, y_pred_nb))

Z=accuracy_score(y_train, nb.predict(x_train))
#acuuracy for test
acc_NB= accuracy_score(y_test, y_pred_nb)
print(acc_NB)

"""## **Apply Hard voting**"""

hard_votting = VotingClassifier(estimators=[('rl', rl), ('ada', ada), ('nb', nb)], voting='hard')
hard_votting1 = hard_votting.fit(x_train, y_train)
print('VotingClassifierModel Train Score: ' , hard_votting.score(x_train, y_train),"\n")

#Calculating Prediction
y_pred1 = hard_votting1.predict(x_test)
acc_votting_hard= accuracy_score(y_test, y_pred1)
print('VotingClassifierModel Test Score: ' , acc_votting_hard)
#Calculating Confusion Matrix
CM = confusion_matrix(y_test, y_pred1)
print('Confusion Matrix is : \n', CM)

print(classification_report(y_test, y_pred1))

"""## **Apply soft voting**"""

wRF = X/(X+Y+Z)
wAdaboost = Y/(X+Y+Z)
wNB = Z/(X+Y+Z)
weights=[wRF,wAdaboost,wNB ]

ensemble = VotingClassifier(estimators=[('rl', rl), ('ada', ada), ('nb', nb)], voting='soft',weights=weights)
ensemble1 = ensemble.fit(x_train, y_train)

#Calculating Details
print('VotingClassifierModel Train Score is : ' , ensemble.score(x_train, y_train),"\n")

#Calculating Prediction
y_pred2 = ensemble1.predict(x_test)
acc_votting_soft= accuracy_score(y_test, y_pred2)
print('VotingClassifierModel Test Score is : ' , acc_votting_soft,"\n")

#Calculating Confusion Matrix
CM = confusion_matrix(y_test, y_pred2)
print('Confusion Matrix is : \n', CM)

print(classification_report(y_test, y_pred2))

"""## **Apply weighted sum aggregation**"""

agr_out = y_pred_rl *wRF + y_pred_ada *wAdaboost + y_pred_nb * wNB
agr_out

def hard_class(x):
  l = []
  for i in x:
    if i>0.5:
      l.append(1)
    else:
      l.append(0)
  return np.array(l)

agr_out= hard_class(agr_out)
agg_acc = accuracy_score(y_test,agr_out)
agg_acc

"""## **Plot a barchart figure**"""

names= ['Random forest', 'AdaBoost' , 'Naive Bayes','Voting Classifiers' , 'ensemble Classifier weighted']
values = [acc_RL , acc_ADA , acc_NB  , acc_votting_hard , agg_acc]

plt.figure(figsize=(10 , 8))
sns.barplot(x=names, y = values)
plt.title('compare between all classifiers accuracy')
plt.xlabel('classifiers')
plt.ylabel('accuracy score')
plt.legend()

"""As we can see, the Random Forest classifier model has the highest accuracy.

## **Compare between Random forest, AdaBoost and GaussianNB classifiers**
"""

plt.figure(figsize=(10, 8))
sns.barplot(x=names[:3], y = values[:3])
plt.title('compare between Random forest , naive bayes and adaboost')
plt.xlabel('classifiers')
plt.ylabel('accuracy score')
plt.legend()

"""## **Comparing between voting and weighted ensemble**"""

plt.figure(figsize=(8 , 6))
sns.barplot(x=names[3:], y = values[3:])
plt.title('compare between voting and weighted ensemble')
plt.xlabel('classifiers')
plt.ylabel('accuracy score')
plt.legend()

"""The weighted ensemble model has the same accuracy score like voting classifier"""